{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Assignment: Code-Focused Inference\n",
                "\n",
                "**Objective:** Build a system using a pre-trained GPT-2 model that answers **only** Python coding questions and rejects everything else."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Install and Import Dependencies\n",
                "!pip install transformers torch\n",
                "\n",
                "import torch\n",
                "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Load Model and Tokenizer\n",
                "# We use the small 'gpt2' model for efficiency.\n",
                "model_name = \"gpt2\"\n",
                "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
                "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
                "\n",
                "# Ensure pad token is set (GPT-2 does not have one by default)\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "model.config.pad_token_id = tokenizer.eos_token_id"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 3. Implement Filtering & Generation Mechanism\n",
                "\n",
                "We will use **Few-Shot Prompting** to guide the model. By providing examples of how to handle coding vs. non-coding questions, the model learns the pattern.\n",
                "\n",
                "**Pattern:**\n",
                "- Non-coding input -> \"I can only answer Python coding questions.\"\n",
                "- Coding input -> [Code Solution]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def ask_python_bot(question, max_new_tokens=50):\n",
                "    \"\"\"\n",
                "    Generates a response if the question is about Python, otherwise refuses.\n",
                "    \"\"\"\n",
                "    \n",
                "    # Few-shot prompt to teach the behavior\n",
                "    prompt = f\"\"\"\n",
                "User: What is the capital of France?\n",
                "Assistant: I can only answer Python coding questions.\n",
                "\n",
                "User: How do I print hello in Python?\n",
                "Assistant: print(\"Hello World\")\n",
                "\n",
                "User: Who won the world cup?\n",
                "Assistant: I can only answer Python coding questions.\n",
                "\n",
                "User: How do I create a list?\n",
                "Assistant: my_list = [1, 2, 3]\n",
                "\n",
                "User: {question}\n",
                "Assistant:\"\"\"\n",
                "\n",
                "    # Tokenize input\n",
                "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
                "    \n",
                "    # Generate response\n",
                "    # We use temperature=0.1 to make it deterministic and focused\n",
                "    with torch.no_grad():\n",
                "        outputs = model.generate(\n",
                "            **inputs,\n",
                "            max_new_tokens=max_new_tokens,\n",
                "            pad_token_id=tokenizer.eos_token_id,\n",
                "            temperature=0.1,  # Low temperature to be less creative/random\n",
                "            do_sample=True,\n",
                "            stop_strings=[\"\\nUser:\"], # Stop generating if it tries to continue the conversation\n",
                "            tokenizer=tokenizer\n",
                "        )\n",
                "    \n",
                "    # Decode output\n",
                "    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "    \n",
                "    # Extract only the assistant's new response\n",
                "    # The model might repeat the prompt, so we strip it.\n",
                "    response_text = full_text[len(prompt):].strip()\n",
                "    \n",
                "    # Cleanup: sometimes it might generate extra newlines or \"User:\"\n",
                "    if \"User:\" in response_text:\n",
                "        response_text = response_text.split(\"User:\")[0].strip()\n",
                "        \n",
                "    return response_text"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 4. Test the Model\n",
                "We will test with a mix of Python and non-Python questions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "test_propmts = [\n",
                "    \"How do I write a for loop in Python?\",\n",
                "    \"What is the tallest mountain?\",\n",
                "    \"How to define a function?\",\n",
                "    \"Tell me a joke about cats.\",\n",
                "    \"import numpy as np\"\n",
                "]\n",
                "\n",
                "print(f\"{'Question':<40} | {'Response'}\")\n",
                "print(\"-\"*80)\n",
                "\n",
                "for q in test_propmts:\n",
                "    response = ask_python_bot(q)\n",
                "    print(f\"{q:<40} | {response}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}